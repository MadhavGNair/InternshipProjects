{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TO-DO:\n",
    "Prompt GPT to generate a ~2000 word report using 5-15 questions from either one of the ESG topics. Leak 1-2 questions from the other\n",
    "topics as well. Store all these ground-truth questions along with the paragraph in a JSON file. Use the following format:\n",
    "[\n",
    "{\n",
    "    \"text\" : (str) the generated text,\n",
    "    \"main_topic\": (str) the main topic,\n",
    "    \"main_ground_truth\": (Dict) the questions relating to main topic,\n",
    "    \"leak_topic\": (str) the leak topic,\n",
    "    \"leak_ground_truth\" : (Dict) the questions relating to leak topic,\n",
    "    \"ground_truth\": (List) all the questions as a list,\n",
    "    \"input_tokens\": (int) number of input tokens,\n",
    "    \"output_tokens\": (int) number of output tokens\n",
    "}\n",
    "]\n",
    "\n",
    "The ground_truth dictionary must follow the format:\n",
    "\n",
    "{\n",
    "\"main_topic\": {\n",
    "    \"sub_topic_1\": [list of questions],\n",
    "    ...,\n",
    "    }\n",
    "\"leak_topic\": None or Dict in the same format as main_topic\n",
    "}\n",
    "\n",
    "For each of the topics in ESG, repeat this process about 6-7 times. End with ~20 total paragraphs.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain imports\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "# general imports\n",
    "import json \n",
    "import random\n",
    "import openai\n",
    "from typing import Dict, Any, List\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = \"API KEY HERE\"\n",
    "MODEL=\"gpt-4o-mini\"\n",
    "\n",
    "client = openai.OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load the atomic query dictionary\n",
    "def load_json(file_path):\n",
    "    json_data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        json_data.append(data)\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to write list of dictionaries to a json\n",
    "def write_to_json(data, file_path):\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read jsonl file\n",
    "def read_jsonl(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            json_object = json.loads(line.strip())\n",
    "            data.append(json_object)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esg_data = load_json('./atomic_factors/ESG_factors.json')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_questions(questions: Dict[str, Dict[str, List[str]]]) -> Dict[str, Any]:\n",
    "    '''\n",
    "    Function to select questions from the entire dataset. The function works by selecting a random main topic from the following\n",
    "    choices: \"Environment\", \"Social\", or \"Governance\". The function also selects a random number between 5 and 15 as the total\n",
    "    number of questions. Given the main topic, the model then iterates through all sub-topics and selects random questions between\n",
    "    each sub-topic. The distribution of questions from sub-topics is not uniform, i.e. questions from every sub-topic might not be \n",
    "    present. The model also selects 1 or 2 leak question from a topic that is not the main topic 30% of the time. The model returns\n",
    "    a dictionary in the following format:\n",
    "    \n",
    "    result = {\n",
    "        \"main_topic\": main topic,\n",
    "        \"main_ground_truth\": questions from the main topic as a Dict of sub-topics,\n",
    "        \"leak_topic\": None or leak topic,\n",
    "        \"leak_ground_truth\": None or questions from the leak topic as a Dict of sub-topics,\n",
    "        \"ground_truth\": list of all questions for easy access\n",
    "    } \n",
    "    '''\n",
    "    main_topics = [\"Environment\", \"Social\", \"Governance\"]\n",
    "    main_topic = random.choice(main_topics)\n",
    "    num_questions = random.randint(5, 8)\n",
    "    \n",
    "    main_ground_truth = {}\n",
    "    ground_truth = []\n",
    "    \n",
    "    # select questions for the main topic\n",
    "    while len(ground_truth) < num_questions:\n",
    "        for sub_topic, sub_questions in questions[main_topic].items():\n",
    "            selected = random.sample(sub_questions, random.randint(0, num_questions - len(ground_truth)))\n",
    "            if selected:\n",
    "                main_ground_truth[sub_topic.replace('.txt', '')] = selected\n",
    "                ground_truth.extend(selected)\n",
    "    \n",
    "    # prepare the result dictionary\n",
    "    result = {\n",
    "        \"main_topic\": main_topic,\n",
    "        \"main_ground_truth\": main_ground_truth,\n",
    "        \"leak_topic\": None,\n",
    "        \"leak_ground_truth\": None,\n",
    "        \"ground_truth\": ground_truth\n",
    "    }\n",
    "    \n",
    "    # 30% chance to have a leak topic\n",
    "    if random.random() < 0.3:\n",
    "        leak_topic = random.choice([t for t in main_topics if t != main_topic])\n",
    "        result[\"leak_topic\"] = leak_topic\n",
    "        \n",
    "        leak_ground_truth = {}\n",
    "        num_leak_questions = random.randint(1, 2)\n",
    "        \n",
    "        for sub_topic, sub_questions in questions[leak_topic].items():\n",
    "            selected = random.sample(sub_questions, min(len(sub_questions), num_leak_questions - len(leak_ground_truth)))\n",
    "            if selected:\n",
    "                leak_ground_truth[sub_topic.replace('.txt', '')] = selected\n",
    "                ground_truth.extend(selected)\n",
    "            if len(leak_ground_truth) >= num_leak_questions:\n",
    "                break\n",
    "        \n",
    "        result[\"leak_ground_truth\"] = leak_ground_truth\n",
    "        result[\"ground_truth\"] = ground_truth\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(selected: Dict[str, Any]) -> str:\n",
    "    prompt = f\"Generate a text of approximately 2000 words (plus or minus 200 words) that results in an excerpt from a listed company's disclosure report that will become avaible to investors and regulators. The text should also indirectly answer the following topics (in the form of questions) with supportive statements and actions that have taken place in the last fiscal year that will allow investors and regulatory to accurately determine that the company has definitively complied in a couple of measure steps. The answers when interpreted will in the regulators' mind check a 'Yes' without explicitly mentioning them:\\n\\n\"\n",
    "    for q in selected['ground_truth']:\n",
    "        prompt += f\"- {q}\\n\"\n",
    "    prompt += f\"\\nThe main focus should be on {selected['main_topic']}.\"\n",
    "    if selected['leak_topic']:\n",
    "        prompt += f\" Also, subtly incorporate some elements related to {selected['leak_topic']}.\"\n",
    "    prompt += \"\\nEnsure the text is complex and realistic, and avoid repeating key words from the questions. Do not have headings relating to each question but rather have paragraphs of texts that transition naturally.\"\n",
    "    prompt += \"\\nDo not dedicate paragraphs to each question but rather mix around the topics and also add text related to the topic that do not answer the questions.\"\n",
    "    prompt += \"\\nFormat the output text as a string as if it was the output of a PDF parser and not in markdown format. For example, replace new lines with '\\\\n' character.\"\n",
    "    prompt += \"\\nRandomly inject mild to severe parsing errors in the textto mimic realistic tools like pypdf2 or pdfminer.\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create the input batches files\n",
    "def create_batch_input(final_list: List[Dict[str, Any]], model_name: str) -> List[Dict[str, Any]]:\n",
    "\n",
    "    batch_input = []\n",
    "    for i, item in enumerate(final_list):\n",
    "        custom_id = f\"request-{i+1}\"\n",
    "        item[\"id\"] = custom_id\n",
    "        batch_input.append({\n",
    "            \"custom_id\": custom_id,\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": model_name,\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"You are an expert AI trained to generate annual report excerpts following the prompt carefully.\"},\n",
    "                    {\"role\": \"user\", \"content\": item[\"prompt\"]}\n",
    "                ],\n",
    "                \"max_tokens\": 3000\n",
    "            }\n",
    "        })\n",
    "    return batch_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output = []\n",
    "\n",
    "for i in range(21):\n",
    "    final_schema = {\n",
    "        \"id\": \"\",\n",
    "        \"text\" : \"\",\n",
    "        \"main_topic\": \"\",\n",
    "        \"main_ground_truth\": {},\n",
    "        \"leak_topic\": \"\",\n",
    "        \"leak_ground_truth\" : {},\n",
    "        \"ground_truth\": [],\n",
    "        \"prompt\": \"\",\n",
    "        \"input_tokens\": 0,\n",
    "        \"output_tokens\": 0\n",
    "    }\n",
    "\n",
    "    ground_truth = select_questions(esg_data)\n",
    "\n",
    "    final_schema[\"main_topic\"] = ground_truth[\"main_topic\"]\n",
    "    final_schema[\"main_ground_truth\"] = ground_truth[\"main_ground_truth\"]\n",
    "    final_schema[\"leak_topic\"] = ground_truth[\"leak_topic\"]\n",
    "    final_schema[\"leak_ground_truth\"] = ground_truth[\"leak_ground_truth\"]\n",
    "    final_schema[\"ground_truth\"] = ground_truth[\"ground_truth\"]\n",
    "\n",
    "    prompt = generate_prompt(ground_truth)\n",
    "    final_schema[\"prompt\"] = prompt\n",
    "\n",
    "    final_output.append(final_schema)\n",
    "\n",
    "\n",
    "input_batches = create_batch_input(final_output, model_name=MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_gen_batches.jsonl', 'w') as file:\n",
    "    for request in input_batches:\n",
    "        file.write(json.dumps(request) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload the batch input file\n",
    "batch_input_file = client.files.create(\n",
    "  file=open(\"data_gen_batches.jsonl\", \"rb\"),\n",
    "  purpose=\"batch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input_file_id = batch_input_file.id\n",
    "\n",
    "batch = client.batches.create(\n",
    "    input_file_id=batch_input_file_id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\",\n",
    "    metadata={\n",
    "      \"description\": \"generate synthetic data\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_output = client.batches.retrieve(batch.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_output.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if batch_output.status == \"completed\":\n",
    "    output = client.files.content(batch_output.output_file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output.write_to_file('raw_batching_output.jsonl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'raw_batching_output.jsonl'\n",
    "jsonl_data = read_jsonl(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_input = 0\n",
    "total_output = 0\n",
    "\n",
    "for idx, jsonl in enumerate(jsonl_data):\n",
    "    output_id = jsonl[\"custom_id\"]\n",
    "    if final_output[idx]['id'] == output_id:\n",
    "        final_output[idx][\"text\"] = jsonl['response']['body']['choices'][0]['message']['content']\n",
    "        final_output[idx][\"input_tokens\"] = jsonl['response']['body']['usage']['prompt_tokens']\n",
    "        final_output[idx][\"output_tokens\"] = jsonl['response']['body']['usage']['completion_tokens']\n",
    "    else:\n",
    "        for item in final_output:\n",
    "            if item['id'] == output_id:\n",
    "                final_output[idx][\"text\"] = jsonl['response']['body']['choices'][0]['message']['content']\n",
    "                final_output[idx][\"input_tokens\"] = jsonl['response']['body']['usage']['prompt_tokens']\n",
    "                final_output[idx][\"output_tokens\"] = jsonl['response']['body']['usage']['completion_tokens']\n",
    "    total_input += jsonl['response']['body']['usage']['prompt_tokens']\n",
    "    total_output += jsonl['response']['body']['usage']['completion_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the output\n",
    "write_to_json(final_output, 'processed_output.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_input, total_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semantic-search-De7fcIV1-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
